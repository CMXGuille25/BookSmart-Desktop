import * as faceDetection from '@tensorflow-models/face-detection';
import * as tf from '@tensorflow/tfjs';
import { useEffect, useRef, useState } from 'react';

const CameraStream = () => {
  const videoRef = useRef(null);
  const streamRef = useRef(null);
  const detectorRef = useRef(null);
  const canvasRef = useRef(null);
  const bibliotecaId = localStorage.getItem('biblioteca');
  const [isStreaming, setIsStreaming] = useState(false);
  const [faceDetected, setFaceDetected] = useState(false);
  
  // Add cooldown state to prevent too many captures
  const lastCaptureTime = useRef(0);
  const CAPTURE_COOLDOWN = 3000; // 3 seconds between captures

  // Function to capture current frame and send to API
  const captureAndSendFrame = async () => {
    if (!videoRef.current) {
      console.error("âŒ Cannot capture: video element not available");
      return;
    }

    // Check if video is ready and has dimensions
    if (videoRef.current.readyState < 2 || videoRef.current.videoWidth === 0) {
      console.error("âŒ Cannot capture: video not ready yet", {
        readyState: videoRef.current.readyState,
        videoWidth: videoRef.current.videoWidth,
        videoHeight: videoRef.current.videoHeight
      });
      return;
    }

    try {
      // Create canvas to capture frame
      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');
      
      // Set canvas dimensions to match video
      canvas.width = videoRef.current.videoWidth;
      canvas.height = videoRef.current.videoHeight;
      
      // Draw current video frame to canvas
      ctx.drawImage(videoRef.current, 0, 0, canvas.width, canvas.height);
      
      // Convert to blob
      canvas.toBlob(async (blob) => {
        if (!blob) {
          console.error("âŒ Failed to create image blob");
          return;
        }

        // Create FormData to send image and biblioteca_id
        const formData = new FormData();
        formData.append('imagen', blob, 'capture.jpg'); // Changed from 'image' to 'imagen'
        formData.append('biblioteca_id', parseInt(bibliotecaId)); // Changed from 'bibliotecaId' to 'biblioteca_id' and convert to int
        
        // Optional: Add tolerance parameter (defaults to 0.6 in API)
        formData.append('tolerance', '0.6');

        try {
          console.log("ğŸ“¤ Sending capture to API...", {
            biblioteca_id: bibliotecaId,
            imageSize: blob.size,
            imageType: blob.type
          });

          const response = await fetch('/api/v1/rostro/verificar', {
            method: 'POST',
            body: formData // Don't add Content-Type header - let browser set it for FormData
          });

          if (response.ok) {
            const result = await response.json();
            console.log("âœ… Capture sent successfully:", result);
            
            // Handle the response based on the API structure
            if (result.match_found) {
              console.log("ğŸ¯ Face match found:", {
                user_id: result.match_info?.user_id,
                confidence: result.match_info?.confidence,
                message: result.message
              });
            } else {
              console.log("âŒ No face match:", result.message);
            }
            
            console.log("ğŸ“· Photo saved:", {
              s3_url: result.s3_url,
              historial_id: result.historial_id,
              deteccion_id: result.deteccion_id
            });
            
          } else {
            const errorText = await response.text();
            console.error("âŒ API error:", response.status, response.statusText, errorText);
          }
        } catch (apiError) {
          console.error("âŒ Network error sending capture:", apiError);
        }
      }, 'image/jpeg', 0.8); // JPEG with 80% quality

    } catch (error) {
      console.error("âŒ Error capturing frame:", error);
    }
  };

  // Initialize TensorFlow and face detector
  const initializeDetector = async () => {
    try {
      // Initialize TensorFlow backend
      await tf.setBackend('webgl');
      await tf.ready();
      console.log("ğŸš€ TensorFlow initialized with backend:", tf.getBackend());

      const model = faceDetection.SupportedModels.MediaPipeFaceDetector;
      const detectorConfig = {
        runtime: 'tfjs',
        maxFaces: 5, // Increased to catch more faces
        modelType: 'short', // Use 'short' for faster detection
        minDetectionConfidence: 0.3, // Lower threshold for better detection
        minSuppressionThreshold: 0.3
      };
      
      detectorRef.current = await faceDetection.createDetector(model, detectorConfig);
      console.log("ğŸ­ Face detector initialized with config:", detectorConfig);
    } catch (err) {
      console.error("âŒ Error initializing TensorFlow:", err);
      throw err;
    }
  };

  // Detect faces in video stream
  const detectFaces = async () => {
    if (!videoRef.current || !detectorRef.current) {
      console.log("ğŸš« Detection skipped - missing requirements:", {
        hasVideo: !!videoRef.current,
        hasDetector: !!detectorRef.current,
        isStreaming: isStreaming
      });
      return;
    }

    // Make sure video is actually playing and has dimensions
    if (videoRef.current.readyState < 2 || videoRef.current.videoWidth === 0) {
      console.log("â³ Video not ready yet:", {
        readyState: videoRef.current.readyState,
        videoWidth: videoRef.current.videoWidth,
        videoHeight: videoRef.current.videoHeight,
        currentTime: videoRef.current.currentTime,
        paused: videoRef.current.paused
      });
      setTimeout(() => {
        requestAnimationFrame(detectFaces);
      }, 500);
      return;
    }

    try {
      const faces = await detectorRef.current.estimateFaces(videoRef.current, {
        flipHorizontal: false,
        staticImageMode: false
      });

      const hasFace = faces.length > 0;
      
      if (hasFace) {
        const face = faces[0];
        console.log("ğŸ‘¤ Face detected:", {
          confidence: face.score?.toFixed(3),
          box: face.box,
          keypoints: face.keypoints?.length || 0,
          videoInfo: {
            width: videoRef.current.videoWidth,
            height: videoRef.current.videoHeight,
            currentTime: videoRef.current.currentTime
          }
        });
      }

      setFaceDetected(hasFace);
      
      // Check if we should capture (new face detected + cooldown expired)
      const currentTime = Date.now();
      const timeSinceLastCapture = currentTime - lastCaptureTime.current;
      
      if (hasFace && !faceDetected) {
        console.log("âœ… New face detected!");
        
        // Only capture if cooldown period has passed
        if (timeSinceLastCapture >= CAPTURE_COOLDOWN) {
          console.log("ğŸ“¸ Capturing photo (cooldown expired)...");
          lastCaptureTime.current = currentTime;
          captureAndSendFrame();
        } else {
          const remainingCooldown = Math.ceil((CAPTURE_COOLDOWN - timeSinceLastCapture) / 1000);
          console.log(`â³ Capture on cooldown (${remainingCooldown}s remaining)`);
        }
      }

      // Continue detection loop at ~15 FPS (67ms delay)
      setTimeout(() => {
        requestAnimationFrame(detectFaces);
      }, 67); // ~15 FPS (1000ms / 15 â‰ˆ 67ms)

    } catch (err) {
      console.error("âŒ Face detection error:", err);
      console.log("ğŸ”§ Error details:", {
        message: err.message,
        stack: err.stack,
        videoElement: {
          readyState: videoRef.current?.readyState,
          videoWidth: videoRef.current?.videoWidth,
          videoHeight: videoRef.current?.videoHeight
        }
      });
      // Retry detection after error
      setTimeout(() => {
        requestAnimationFrame(detectFaces);
      }, 1000);
    }
  };

  useEffect(() => {
    const startStream = async () => {
      try {
        await initializeDetector();

        const devices = await navigator.mediaDevices.enumerateDevices();
        const videoInputs = devices.filter(device => device.kind === 'videoinput');
        console.log("ğŸ“· Available video inputs:", videoInputs.map(d => d.label));

        const usbCamera = videoInputs.find(device =>
          !device.label.toLowerCase().includes('integrated') &&
          !device.label.toLowerCase().includes('built-in')
        );

        if (!usbCamera) {
          console.error('âŒ USB camera not found');
          throw new Error('USB camera not found');
        }

        console.log("âœ… Selected camera:", usbCamera.label);

        const stream = await navigator.mediaDevices.getUserMedia({
          video: { 
            deviceId: { exact: usbCamera.deviceId },
            width: { ideal: 1280 },
            height: { ideal: 720 }
          }
        });

        if (videoRef.current) {
          videoRef.current.srcObject = stream;
          streamRef.current = stream;
          
          // Wait for video to be ready before playing
          videoRef.current.onloadedmetadata = async () => {
            try {
              console.log("ğŸ“¹ Video metadata loaded, dimensions:", {
                width: videoRef.current.videoWidth,
                height: videoRef.current.videoHeight,
                readyState: videoRef.current.readyState
              });
              
              await videoRef.current.play();
              console.log("ğŸ¥ Stream started successfully");
              
              // Set streaming state first
              setIsStreaming(true);
              
              // Wait a bit more for the video to stabilize and state to update before starting detection
              setTimeout(() => {
                console.log("ğŸ” Starting face detection...");
                detectFaces();
              }, 1500); // Increased wait time to ensure state update
            } catch (playError) {
              console.error("Error playing video:", playError);
            }
          };
          
          // Add error handler for video element
          videoRef.current.onerror = (e) => {
            console.error("Video element error:", e);
          };
        }

      } catch (err) {
        console.error('Error starting video stream:', err.message);
        setIsStreaming(false);
      }
    };

    startStream();

    return () => {
      if (streamRef.current) {
        const tracks = streamRef.current.getTracks();
        tracks.forEach(track => {
          console.log("Stopping track:", track.label);
          track.stop();
        });
        if (videoRef.current) {
          videoRef.current.srcObject = null;
        }
        setIsStreaming(false);
        setFaceDetected(false);
        console.log("ğŸ›‘ Stream stopped and cleaned up");
      }
    };
  }, []);

  return (
    <>
      <video 
        ref={videoRef}
        autoPlay
        playsInline
        muted
        width="640"
        height="480"
        style={{ 
          display: 'none', // Hidden but still functional for face detection
          position: 'fixed',
          zIndex: -1
        }}
      />
      {isStreaming && (
        <div
          style={{
            position: 'fixed',
            top: '10px',
            right: '10px',
            display: 'flex',
            alignItems: 'center',
            gap: '5px',
            background: 'rgba(0, 0, 0, 0.7)',
            padding: '5px 10px',
            borderRadius: '15px',
            color: 'white',
            fontSize: '12px',
            zIndex: 9999,
          }}
        >
          <div
            style={{
              width: '8px',
              height: '8px',
              backgroundColor: faceDetected ? '#00ff00' : '#ff0000',
              borderRadius: '50%',
              animation: 'pulse 2s infinite'
            }}
          />
          <style>
            {`
              @keyframes pulse {
                0% { opacity: 1; }
                50% { opacity: 0.5; }
                100% { opacity: 1; }
              }
            `}
          </style>
          <span>CÃ¡mara activa {faceDetected ? '(Rostro detectado)' : ''}</span>
        </div>
      )}
    </>
  );
};

export default CameraStream;
